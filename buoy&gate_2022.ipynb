{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] Libraries that are commented out are only for ROS\n",
    "#import rospy\n",
    "#from std_msgs.msg import Float64\n",
    "#from sensor_msgs.msg import Image\n",
    "#from cv_bridge import CvBridge, CvBridgeError\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import imutils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Gate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "\n",
    "current_yaw = 0\n",
    "cv_image = 0\n",
    "YAW_VARIANCE = .017\n",
    "\n",
    "# [+]- Access the camera, the int parameter determines which camera you are using, may have to change in depending on what computer you run on\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "# video_capture = cv2.VideoCapture(1)\n",
    "# video_capture = cv2.VideoCapture(2)\n",
    "\n",
    "#[+]- Each iteration of this loop processes a frame that is captured by the camera by applying a series of filters. Each filter is in intermediate step, the final image is the one which we annotate and extract information from. \n",
    "while(True):\n",
    "    # Capture the raw frame\n",
    "    ret, frame = video_capture.read()\n",
    "    final = frame\n",
    "\n",
    "    # change the frame from bgr values to hsv values\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # [+] specify the lower and upper bounds of colors to be NOT filtered out\n",
    "    lower_color_bounds = np.array([0, 100, 20])\n",
    "    upper_color_bounds = np.array([30, 255, 255])\n",
    "\n",
    "    # [+]- Filter 1: Take the frame, filter out the pixels that are NOT included in the lower and upper bounds, threshold shows in black the pixels being filtered out, and the pixels that are included are white\n",
    "    threshold = cv2.inRange(hsv, lower_color_bounds, upper_color_bounds)\n",
    "\n",
    "    # [+]- Filter 2:  Take the thresholded image, and erode to remove noise\n",
    "    kernel = np.ones((10, 10), np.uint8)\n",
    "    erode = cv2.erode(threshold, kernel)\n",
    "\n",
    "    # Get the contours from the eroded image, Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object\n",
    "    contours = cv2.findContours(erode, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[0]\n",
    "\n",
    "    # Get the areas of each contour. Store each contour area in an array, \n",
    "    contour_areas = [cv2.contourArea(x) for x in contours]  \n",
    "    \n",
    "    # Sort the array of areas from smallest to largest, returns a list of indeces for contour areas array \n",
    "    contour_indexes = np.argsort(contour_areas)  \n",
    "    \n",
    "    # Sort the indexes of the largest areas\n",
    "    for i in contour_indexes[-2:]:  # only look at the two largest contours\n",
    "        (x,y,w,h) = cv2.boundingRect(contours[i])  # get the location/dimensions of a bounding box for the contour: x,y=coordinates, w,h=dims\n",
    "        cv2.rectangle(final, pt1=(x,y), pt2=(x+w,y+h), color=(255,0,0), thickness=5)  # draw the bounding box on the image\n",
    "\n",
    "        # for visibility, we will place a background fill on the contour label\n",
    "        text = \"gatepost\"\n",
    "        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n",
    "        text_w, text_h = text_size\n",
    "        cv2.rectangle(final, pt1=(x, y), pt2=(x + text_w, y - 2*text_h), color=(255, 0, 0), thickness=-1)\n",
    "        cv2.putText(final, \"gatepost\", org=(x, y-5), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=1, color=(255, 255, 255), thickness=1)\n",
    "\n",
    "    # [+]- Show each filter that is applied to the frame. Each filter is shown in a separate window\n",
    "    cv2.imshow('1 Gate: original frame',frame)\n",
    "    cv2.imshow('2 Gate: threshold', threshold)\n",
    "    cv2.imshow(\"3 Gate: eroded\", erode)\n",
    "    cv2.imshow(\"4 Gate: final\", final)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "#[+]:\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Buoy__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "\n",
    "current_yaw = 0\n",
    "cv_image = 0\n",
    "YAW_VARIANCE = .017\n",
    "\n",
    "# [+]- Access the camera, camNum determines which camera you are using, may have to change in depending on what computer you run on\n",
    "camNum= 0\n",
    "video_capture = cv2.VideoCapture(camNum)\n",
    "\n",
    "\n",
    "#[+]- Each iteration of this loop processes a frame that is captured by the camera by applying a series of filters. Each filter is in intermediate step, the final image is the one which we annotate and extract information from. \n",
    "while(True):\n",
    "    # Capture the raw frame\n",
    "    ret, img_frame = video_capture.read()\n",
    "    img_final = img_frame\n",
    "\n",
    "    # change the frame from bgr values to hsv values\n",
    "    img_hsv = cv2.cvtColor(img_frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\n",
    "\n",
    "#[+]======================================================================================================================================================\\\\\n",
    "    # [+] Filter 1: Take the frame, filter out the pixels that are NOT included in the lower and upper bounds, threshold shows in black the pixels being filtered out, and the pixels that are included are white\n",
    "    lower_value_bounds = np.array([0, 0, 0+20])\n",
    "    upper_value_bounds = np.array([255, 255, 45+20])\n",
    "    img_threshold = cv2.inRange(img_hsv, lower_value_bounds, upper_value_bounds)\n",
    "\n",
    "    #[+] Filter 2: Take the threshold image and dilate it to make en thicker\n",
    "    dilateKernel = np.ones((5, 5), np.uint8)\n",
    "    img_dilate = cv2.dilate(img_threshold, dilateKernel, iterations=3)\n",
    "\n",
    "    # [+]- Filter 3:  Take the dilated image, and erode it to remove noise\n",
    "    erodeKernel = np.ones((10, 10), np.uint8)\n",
    "    img_erode = cv2.erode(img_dilate, erodeKernel, iterations=2)\n",
    "\n",
    "    #[+] Filter 4: Take the erode image and blur to to smooth edges so circle detection is easier. At this point, the circle, like the one on the badge, should be extra visible\n",
    "    blurnel = (5, 5)\n",
    "    img_blur = cv2.blur(img_erode, ksize= blurnel)\n",
    "\n",
    "    #[+] Circle detection: Take the blurred image, detect any circles in may have, and store them in this 'circles' array. Each circle is represented as an array of the form [x, y, radius]\n",
    "    circles = cv2.HoughCircles(img_blur, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                  param1=30,  # edge detection parameter\n",
    "                  param2=30,  # accumulator threshold, or how circley the circles need to be to be recognized (higher=more circlely)\n",
    "                  minRadius=0,\n",
    "                  maxRadius=100)\n",
    "#[+]=====================================================================================================================================================//\n",
    "\n",
    "\n",
    "\n",
    "    # [+] Check if the circles array is not empty, if so, render over every circle in the array of circles:\n",
    "    if (type(circles)) is np.ndarray:\n",
    "\n",
    "        #[+] Get the radius of each circle and sort indeces from smallest to largest\n",
    "        circle_radii = [x[2] for x in circles[0]]  \n",
    "        circle_indexes = np.argsort(circle_radii)  \n",
    "\n",
    "        #[+] Render every circle in the circles array. Negative indicies count from the end to the beginning of the array\n",
    "        for i in circle_indexes[-2:]:  # only contour at the largest circles\n",
    "\n",
    "            #[+] Get the largest circle, specify the parameters, and render the circle: \n",
    "            circle = circles[0][i]  \n",
    "            cv2.circle(img_final, \n",
    "                       center=(int(circle[0]), int(circle[1])), \n",
    "                       radius=int(circle[2]), \n",
    "                       color=(100, 0, 255), \n",
    "                       thickness=2\n",
    "            )  \n",
    "\n",
    "            # [+] Create the text, and get its size and dimensions, and determine the cooridnates of its origin (center)\n",
    "            text = \" -->   <-- \"\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)  \n",
    "            text_w, text_h = text_size     \n",
    "            origX= int(circle[0])-text_w\n",
    "            origY= int(circle[1])+(text_h//2)+8                                     \n",
    "\n",
    "            #[+] Render the text with its size, dimensions, and origin onto the target frame (final frame)\n",
    "            cv2.putText(img_final, text, \n",
    "                org=(origX, origY ), \n",
    "                fontFace=cv2.FONT_HERSHEY_PLAIN, \n",
    "                fontScale=2, \n",
    "                color=(100, 0, 255), \n",
    "                thickness=2\n",
    "            )\n",
    "\n",
    "\n",
    "    # [+] Show each step of the image processing pipeline, each window displays the result of a separate filter\n",
    "    cv2.imshow('1 original frame', img_frame)\n",
    "    cv2.imshow('2 Buoy: hsv', img_hsv)\n",
    "    cv2.imshow('3 Buoy: threshold', img_threshold)\n",
    "    cv2.imshow(\"4 Buoy: dilated\", img_dilate)\n",
    "    cv2.imshow(\"5 Buoy: eroded\", img_erode)\n",
    "    cv2.imshow(\"6 Buoy: blur\", img_blur)\n",
    "    cv2.imshow(\"7 Buoy: final\", img_final)\n",
    "\n",
    "\n",
    "    #[+] When you hit \"q\", it exits the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "#[+] When you exit the loop, it closes the program\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {


   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMP\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10065f837bc8873f5387f5268b450066d881a20043fa6c000aa3f801fbe31849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
